{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm\n",
    "from dataloader_cage import dataformat_for_rnn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class causal_conv(nn.Module):\n",
    "    def __init__(self, cut):\n",
    "        super(causal_conv, self).__init__()\n",
    "        self.cut = cut\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.cut].contiguous()\n",
    "\n",
    "\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0):\n",
    "        super(conv_block, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.causal_conv1 = causal_conv(padding)\n",
    "        self.activate1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.causal_conv2 = causal_conv(padding)\n",
    "        self.activate2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.causal_conv1, self.activate1, self.dropout1,\n",
    "                                 self.conv2, self.causal_conv2, self.activate2, self.dropout2)\n",
    "        if n_inputs != n_outputs:\n",
    "            self.downsample = nn.Conv1d(n_inputs, n_outputs, 1)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        if self.downsample is None:\n",
    "            res = x\n",
    "        else:\n",
    "            res = self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class temp_conv_net(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=4, dropout=0):\n",
    "        super(temp_conv_net, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            if i == 0:\n",
    "                in_channels = num_inputs\n",
    "            else:\n",
    "                in_channels = num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [conv_block(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "class pack_temp_conv_net(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
    "        super(pack_temp_conv_net, self).__init__()\n",
    "        self.tcn = temp_conv_net(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        y1 = self.tcn(inputs)\n",
    "        o = self.linear(y1[:, :, -1])\n",
    "        return torch.squeeze(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_conv_decoder(train_x, train_y, lr, n_channels, filter_size, dropout, epoch, batchSize, checkPoint, use_cuda = False):\n",
    "    lossList = []\n",
    "    # build up data loader, train_x and train_y should be the format for the network\n",
    "    dataset = dataformat_for_rnn(train_x, train_y)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True, sampler=None,\n",
    "                                             batch_sampler=None)\n",
    "    net = None\n",
    "    D_input, D_output = np.size(train_x, 1), np.size(train_y, 1)\n",
    "    \n",
    "    net = pack_temp_conv_net(D_input, D_output, n_channels, filter_size, dropout)\n",
    "    \n",
    "    if use_cuda:\n",
    "        net = net.cuda()\n",
    "    net = net.train()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    t1 = time.time()\n",
    "    lossSum = 0\n",
    "\n",
    "    print(\"Data loader num:\", len(dataloader))\n",
    "\n",
    "    for i in range(epoch):\n",
    "        for batch_idx, (x, y) in enumerate(dataloader):\n",
    "            x, y = x.type('torch.FloatTensor'), y.type('torch.FloatTensor')\n",
    "            if use_cuda:\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = net.forward(x)\n",
    "            loss = criterion(pred, y)\n",
    "            lossSum += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 10 == 0:\n",
    "                if batch_idx % checkPoint == 0 and batch_idx != 0:\n",
    "                   print(\"batch: %d , loss is:%f\" % (batch_idx, lossSum / checkPoint))\n",
    "                   lossList.append(lossSum / checkPoint)\n",
    "                   lossSum = 0\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\"%d epoch is finished!\" % (i+1))\n",
    "\n",
    "    t2 = time.time()\n",
    "    print(\"train time:\", t2-t1)\n",
    "    return net\n",
    "\n",
    "\n",
    "def test_conv_decoder(net, test_x, use_cuda=False):\n",
    "    net = net.eval()\n",
    "    with torch.no_grad():\n",
    "        test_x = torch.from_numpy(test_x).type('torch.FloatTensor')\n",
    "        if use_cuda:\n",
    "            test_x = test_x.cuda()\n",
    "        pred = net(test_x)\n",
    "        if use_cuda:\n",
    "            pred = pred.cpu()\n",
    "    return pred.data.numpy()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import fnmatch, os\n",
    "from xds import lab_data, list_to_nparray, smooth_binned_spikes\n",
    "from IPython.display import clear_output\n",
    "\n",
    "base_path = '../lab_data/Greyson_WM_2D/'\n",
    "file_list = fnmatch.filter(os.listdir(base_path), \"*.mat\")\n",
    "file_list = np.sort(file_list)\n",
    "print(file_list)\n",
    "#%%\n",
    "from util import fix_bad_array, find_EMG_idx\n",
    "bin_size = 0.05\n",
    "bad_chs = []\n",
    "bad_chs = [9, 19, 29, 39, 49, 59, 69, 1, 11, 21, 31, 41, 61, 71, 2, 12, 22, 32, 3, 13, 4, 14, 24, 26, 20, 40]\n",
    "EMG_list = ['EMG_FCR', 'EMG_FDS1', 'EMG_ECR', 'EMG_ECU']\n",
    "\n",
    "file_name = '20191218_Greyson_WM_002.mat'\n",
    "dataset = lab_data(base_path, file_name)\n",
    "dataset.update_bin_data(bin_size)\n",
    "idx_s, idx_e = fix_bad_array(dataset, bad_chs), find_EMG_idx(dataset, EMG_list)\n",
    "train_spike, train_emg = dataset.spike_counts[:, idx_s], dataset.EMG#[:, idx_e]\n",
    "\n",
    "#file_name = \"20190815_Greyson_Key_002.mat\"\n",
    "#file_name = \"Jango_20140725_IsoHandleHoriz_Utah10ImpEMGs_SN_001.mat\"\n",
    "file_name = '20191218_Greyson_WM_003.mat'\n",
    "dataset = lab_data(base_path, file_name)\n",
    "dataset.update_bin_data(0.05)\n",
    "idx_s, idx_e = fix_bad_array(dataset, bad_chs), find_EMG_idx(dataset, EMG_list)\n",
    "test_spike, test_emg = dataset.spike_counts[:, idx_s], dataset.EMG#[:, idx_e]\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17984\n",
      "17984\n",
      "The vaf of Wiener cascade decoder is: 0.592\n",
      "The vaf of Wiener decoder (linear) is: 0.554\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Linear decoder\n",
    "\"\"\"\n",
    "from wiener_filter import dataset_for_WF_multifile\n",
    "from wiener_filter import wiener_cascade_train, wiener_cascade_test, wiener_only_train,w_filter_test\n",
    "from util import vaf\n",
    "\n",
    "n_lags = 10\n",
    "train_x_wiener, train_y_wiener = dataset_for_WF_multifile(train_spike, train_emg, n_lags)\n",
    "print(np.size(train_x_wiener, 0))\n",
    "test_x_wiener, test_y_wiener = dataset_for_WF_multifile(test_spike, test_emg, n_lags)\n",
    "print(np.size(test_x_wiener, 0))\n",
    "\n",
    "H_reg, res_lsq = wiener_cascade_train(train_x_wiener, train_y_wiener, l2 = 0)\n",
    "test_y_wiener_pred = wiener_cascade_test(test_x_wiener, H_reg, res_lsq)\n",
    "print('The vaf of Wiener cascade decoder is: %.3f' % vaf(test_y_wiener, test_y_wiener_pred))\n",
    "\n",
    "H_reg = wiener_only_train(train_x_wiener, train_y_wiener, l2 = 0)\n",
    "test_y_wiener_pred = w_filter_test(test_x_wiener, H_reg)\n",
    "print('The vaf of Wiener decoder (linear) is: %.3f' % vaf(test_y_wiener, test_y_wiener_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17994\n",
      "17994\n",
      "Data loader num: 141\n",
      "batch: 50 , loss is:7.421503\n",
      "batch: 100 , loss is:4.873514\n",
      "1 epoch is finished!\n",
      "batch: 50 , loss is:73.489983\n",
      "batch: 100 , loss is:2.151832\n",
      "11 epoch is finished!\n",
      "batch: 50 , loss is:52.859498\n",
      "batch: 100 , loss is:1.747653\n",
      "21 epoch is finished!\n",
      "batch: 50 , loss is:42.880444\n",
      "batch: 100 , loss is:1.523186\n",
      "31 epoch is finished!\n",
      "train time: 65.67585873603821\n",
      "The vaf of CNN based decoder is: 0.673\n"
     ]
    }
   ],
   "source": [
    "from dataloader_cage import create_samples_xy_rnn_list\n",
    "n_lags = 18\n",
    "train_x, train_y = create_samples_xy_rnn_list(train_spike, train_emg, n_lags, 1)\n",
    "test_x, test_y = create_samples_xy_rnn_list(test_spike, test_emg, n_lags, 1)\n",
    "n_channels = [np.size(train_x,1), np.size(train_x,1), np.size(train_x,1), np.size(train_x,1)]\n",
    "filter_size = 4\n",
    "dropout = 0.05\n",
    "TCN_decoder = train_conv_decoder(train_x, train_y, 0.001, n_channels, filter_size, dropout, \n",
    "                                 epoch = 40, batchSize = 128, checkPoint = 50, use_cuda = True)\n",
    "pred_y = test_conv_decoder(TCN_decoder, test_x, True)\n",
    "print('The vaf of CNN based decoder is: %.3f' % vaf(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17994\n",
      "17994\n",
      "LSTM(70, 100, batch_first=True)\n",
      "Data loader num: 141\n",
      "batch: 50 , loss is:10.441832\n",
      "batch: 100 , loss is:7.039847\n",
      "1 epoch is finished!\n",
      "batch: 50 , loss is:76.908660\n",
      "batch: 100 , loss is:2.040569\n",
      "11 epoch is finished!\n",
      "batch: 50 , loss is:46.637114\n",
      "batch: 100 , loss is:1.550000\n",
      "21 epoch is finished!\n",
      "batch: 50 , loss is:37.623992\n",
      "batch: 100 , loss is:1.272443\n",
      "31 epoch is finished!\n",
      "train time: 18.412959337234497\n",
      "0.6386242883360738\n"
     ]
    }
   ],
   "source": [
    "from decoder_rnn import train_RNN_decoder, predict_RNN_decoder\n",
    "from dataloader_cage import create_samples_xy_rnn_list\n",
    "n_lags = 18\n",
    "train_x, train_y = create_samples_xy_rnn_list(train_spike, train_emg, n_lags, 0)\n",
    "test_x, test_y = create_samples_xy_rnn_list(test_spike, test_emg, n_lags, 0)\n",
    "RNN_decoder = train_RNN_decoder(train_x, train_y, n_lags, 0.001, 'LSTM', hidden_num = 100, n_layer = 1, \n",
    "                      epoch = 40, batchSize = 128, checkPoint = 50, use_cuda = True)\n",
    "test_y_rnn = predict_RNN_decoder(RNN_decoder, test_x, True)\n",
    "print(vaf(test_y, test_y_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
